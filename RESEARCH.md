- [Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation](https://arxiv.org/abs/2403.19285) - Tang et al. | Mar 2024

     Proposes a syntax-based in-context example selection method for MT using dependency tree similarity, and an ensemble strategy combining word-level and syntax-level criteria, improving LLMs' MT performance on 11 out of 12 translation directions.

- [Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning](https://arxiv.org/abs/2403.14399) - Zan | Mar 2024 

    A two-stage fine-tuning algorithm improves LLMs' instruction-following ability, reducing off-target translations and enhancing translation quality across 16 zero-shot directions, while preserving general task performance.

- [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832) - Sun & Miceli-Barone | Mar 2024 

    Larger LLMs may become more susceptible to successful prompt injection attacks in machine translation tasks, an instance of the Inverse Scaling phenomenon, as studied on a new multi-lingual benchmark dataset.

- [Teaching Large Language Models an Unseen Language on the Fly](https://arxiv.org/abs/2402.19167) - Zhang et al. | Feb 2024 

    DiPMT++ framework adapts LLMs to unseen languages through in-context learning, significantly enhancing translation performance for low-resource languages and aiding in the preservation of linguistic diversity.

- [Improving LLM-based Machine Translation with Systematic Self-Correction](https://arxiv.org/abs/2402.16379) - Feng et al. | Feb 2024
    
     The TER (Translate, Estimate, and Refine) framework assists LLMs in improving translation quality across various languages through systematic self-correction, exhibiting superior systematicity and interpretability compared to previous methods.

- [The Future of Machine Translation Lies with Large Language Models](https://arxiv.org/abs/2305.01181) - Anonymous | Feb 2024 

     LLMs offer vast linguistic understanding and innovative methodologies like prompt-based techniques, elevating MT in areas such as long-document translation, stylized translation, and interactive translation, while addressing privacy concerns.

- [Adapting Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2401.06468) - Wu et al. | Feb 2024 

    Adapting LLMs for document-level machine translation can surpass GPT-4 in some cases, but off-target translation issues persist. The study explores prompt strategies, fine-tuning methods, and provides in-depth analysis.

- [Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation](https://arxiv.org/abs/2401.08417) - Xu et al. | Jan 2024 

     Contrastive Preference Optimization (CPO) trains LLMs to avoid generating adequate but imperfect translations, resulting in ALMA-R, which matches or exceeds WMT competition winners and GPT-4 on recent test datasets.

- [Adaptive Machine Translation with Large Language Models](https://arxiv.org/abs/2301.13294) - Moslem et al. | May 2023 

    LLMs' in-context learning capabilities can improve real-time adaptive MT by adapting to domain-specific sentence pairs and terminology, surpassing strong encoder-decoder MT systems, especially for high-resource languages.

- [Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability](https://arxiv.org/abs/2304.04763) - Yamada | Apr 2023 

    Integrating the purpose and target audience into prompts can modify ChatGPT's translations, generally enhancing quality by industry standards and demonstrating the practical application of the "good translation" concept.